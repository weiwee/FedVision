{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FedVision is a Visual Object Detection Platform Powered by Federated Learning At this version, FedVision utilize PaddleFL and PaddleDetection to achieve Gradient Average strategy. This project is in really early stage but we aim to implement many awesome functions such as google's secure aggregation protocol and Gradient compression .","title":"Welcome to Fedvision"},{"location":"deploy/cli/","text":"CLI \u00b6 Usage : $ [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : deploy : deploy tools services : services [start|stop] tools template : template tools deploy \u00b6 deploy tools Usage : $ deploy [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : deploy deploy deploy \u00b6 Usage : $ deploy deploy [ OPTIONS ] Options : --config FILE : [required] --help : Show this message and exit. services \u00b6 services [start|stop] tools Usage : $ services [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : all : [start|stop] all services cluster-manager : [start|stop] cluster manager service cluster-worker : [start|stop] cluster worker service coordinator : [start|stop] coordinator service master : [start|stop] master service services all \u00b6 [start|stop] all services Usage : $ services all [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start all services stop : stop all services services all start \u00b6 start all services Usage : $ services all start [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit. services all stop \u00b6 stop all services Usage : $ services all stop [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit. services cluster-manager \u00b6 [start|stop] cluster manager service Usage : $ services cluster-manager [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster manager stop : stop cluster manager services cluster-manager start \u00b6 start cluster manager Usage : $ services cluster-manager start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit. services cluster-manager stop \u00b6 stop cluster manager Usage : $ services cluster-manager stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit. services cluster-worker \u00b6 [start|stop] cluster worker service Usage : $ services cluster-worker [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster worker stop : stop cluster worker services cluster-worker start \u00b6 start cluster worker Usage : $ services cluster-worker start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME LOCAL_IP PORT_START PORT_END MAX_TASKS CLUSTER_MANAGER_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] LOCAL_IP : local ip [required] PORT_START : port start [required] PORT_END : port start [required] MAX_TASKS : num of maximum parallel tasks [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] Options : --data-dir TEXT : data dir --help : Show this message and exit. services cluster-worker stop \u00b6 stop cluster worker Usage : $ services cluster-worker stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] Options : --help : Show this message and exit. services coordinator \u00b6 [start|stop] coordinator service Usage : $ services coordinator [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start coordinator stop : stop coordinator services coordinator start \u00b6 start coordinator Usage : $ services coordinator start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit. services coordinator stop \u00b6 stop coordinator Usage : $ services coordinator stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit. services master \u00b6 [start|stop] master service Usage : $ services master [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start master stop : stop master services master start \u00b6 start master Usage : $ services master start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT PARTY_ID CLUSTER_MANAGER_ADDRESS COORDINATOR_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] PARTY_ID : party id [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] COORDINATOR_ADDRESS : coordinator address [required] Options : --help : Show this message and exit. services master stop \u00b6 stop master Usage : $ services master stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] Options : --help : Show this message and exit. template \u00b6 template tools Usage : $ template [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : generate : generate template standalone : generate template for standalone deploy template generate \u00b6 generate template Usage : $ template generate [ OPTIONS ] Options : --help : Show this message and exit. template standalone \u00b6 generate template for standalone deploy Usage : $ template standalone [ OPTIONS ] Options : --help : Show this message and exit.","title":"fedvision-deploy-toolkit-cli"},{"location":"deploy/cli/#cli","text":"Usage : $ [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : deploy : deploy tools services : services [start|stop] tools template : template tools","title":"CLI"},{"location":"deploy/cli/#deploy","text":"deploy tools Usage : $ deploy [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : deploy","title":"deploy"},{"location":"deploy/cli/#deploy-deploy","text":"Usage : $ deploy deploy [ OPTIONS ] Options : --config FILE : [required] --help : Show this message and exit.","title":"deploy deploy"},{"location":"deploy/cli/#services","text":"services [start|stop] tools Usage : $ services [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : all : [start|stop] all services cluster-manager : [start|stop] cluster manager service cluster-worker : [start|stop] cluster worker service coordinator : [start|stop] coordinator service master : [start|stop] master service","title":"services"},{"location":"deploy/cli/#services-all","text":"[start|stop] all services Usage : $ services all [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start all services stop : stop all services","title":"services all"},{"location":"deploy/cli/#services-all-start","text":"start all services Usage : $ services all start [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit.","title":"services all start"},{"location":"deploy/cli/#services-all-stop","text":"stop all services Usage : $ services all stop [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit.","title":"services all stop"},{"location":"deploy/cli/#services-cluster-manager","text":"[start|stop] cluster manager service Usage : $ services cluster-manager [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster manager stop : stop cluster manager","title":"services cluster-manager"},{"location":"deploy/cli/#services-cluster-manager-start","text":"start cluster manager Usage : $ services cluster-manager start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit.","title":"services cluster-manager start"},{"location":"deploy/cli/#services-cluster-manager-stop","text":"stop cluster manager Usage : $ services cluster-manager stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit.","title":"services cluster-manager stop"},{"location":"deploy/cli/#services-cluster-worker","text":"[start|stop] cluster worker service Usage : $ services cluster-worker [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster worker stop : stop cluster worker","title":"services cluster-worker"},{"location":"deploy/cli/#services-cluster-worker-start","text":"start cluster worker Usage : $ services cluster-worker start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME LOCAL_IP PORT_START PORT_END MAX_TASKS CLUSTER_MANAGER_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] LOCAL_IP : local ip [required] PORT_START : port start [required] PORT_END : port start [required] MAX_TASKS : num of maximum parallel tasks [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] Options : --data-dir TEXT : data dir --help : Show this message and exit.","title":"services cluster-worker start"},{"location":"deploy/cli/#services-cluster-worker-stop","text":"stop cluster worker Usage : $ services cluster-worker stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] Options : --help : Show this message and exit.","title":"services cluster-worker stop"},{"location":"deploy/cli/#services-coordinator","text":"[start|stop] coordinator service Usage : $ services coordinator [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start coordinator stop : stop coordinator","title":"services coordinator"},{"location":"deploy/cli/#services-coordinator-start","text":"start coordinator Usage : $ services coordinator start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit.","title":"services coordinator start"},{"location":"deploy/cli/#services-coordinator-stop","text":"stop coordinator Usage : $ services coordinator stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit.","title":"services coordinator stop"},{"location":"deploy/cli/#services-master","text":"[start|stop] master service Usage : $ services master [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start master stop : stop master","title":"services master"},{"location":"deploy/cli/#services-master-start","text":"start master Usage : $ services master start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT PARTY_ID CLUSTER_MANAGER_ADDRESS COORDINATOR_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] PARTY_ID : party id [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] COORDINATOR_ADDRESS : coordinator address [required] Options : --help : Show this message and exit.","title":"services master start"},{"location":"deploy/cli/#services-master-stop","text":"stop master Usage : $ services master stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] Options : --help : Show this message and exit.","title":"services master stop"},{"location":"deploy/cli/#template","text":"template tools Usage : $ template [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : generate : generate template standalone : generate template for standalone deploy","title":"template"},{"location":"deploy/cli/#template-generate","text":"generate template Usage : $ template generate [ OPTIONS ] Options : --help : Show this message and exit.","title":"template generate"},{"location":"deploy/cli/#template-standalone","text":"generate template for standalone deploy Usage : $ template standalone [ OPTIONS ] Options : --help : Show this message and exit.","title":"template standalone"},{"location":"develop/codestyle/","text":"We use black and flake8 to format codes in fedvision . For developer, it's quiet easy to install pre-commit hooks: pip install pre-commit pre-commit install This will check code style of changed files before you push codes.","title":"codestyle"},{"location":"framework/overview/","text":"Overview \u00b6 There are two role associated in FedVision : coordinator Party Coordinator is an independent role responsible for handling job publish and distribute subtasks to proper parties that have subscribed to the coordinator . Usually, the coordinator must be started before any party can subscribe to it. The party will post the job of the specified job_type to the coordinator and wait for proposal_waiting_time seconds. While waiting, all party subscribed to this job_type will received a message, and then decide whether to participate or not. After the waiting time is over, the coordinator selects a group of party as a participants in this job. Party is an independent role that publishes or subscribe jobs. usually, It has a Cluster to process assigned tasks, and a submit service to process work requests from Users , a coordinator clients publish jobs to Coordinator or subscribe jobs from \"Coordinator\" and, a master` to mixes them up. Job's Life Cycle \u00b6 The FedVision framework is an extensible framework. When the master receives the job submit request, it will use different job loader according to the job_type parameter. This makes it possible to extent FedVision framework with various machine learning frameworks. Currently, only PaddlePaddle supported with extension configuration file: PaddleFL : jobs : - name : paddle_fl schema : ../schema/paddle_fl.json loader : fedvision.paddle_fl.job:PaddleFLJob tasks : - name : fl_trainer loader : fedvision.paddle_fl.tasks.task.trainer:FLTrainer - name : fl_aggregator loader : fedvision.paddle_fl.tasks.task.aggregator:FLAggregator To implement an new extension, one need to add configuration to extensions.yaml implement abstract job class implement several abstract task class used by job.","title":"framework"},{"location":"framework/overview/#overview","text":"There are two role associated in FedVision : coordinator Party Coordinator is an independent role responsible for handling job publish and distribute subtasks to proper parties that have subscribed to the coordinator . Usually, the coordinator must be started before any party can subscribe to it. The party will post the job of the specified job_type to the coordinator and wait for proposal_waiting_time seconds. While waiting, all party subscribed to this job_type will received a message, and then decide whether to participate or not. After the waiting time is over, the coordinator selects a group of party as a participants in this job. Party is an independent role that publishes or subscribe jobs. usually, It has a Cluster to process assigned tasks, and a submit service to process work requests from Users , a coordinator clients publish jobs to Coordinator or subscribe jobs from \"Coordinator\" and, a master` to mixes them up.","title":"Overview"},{"location":"framework/overview/#jobs-life-cycle","text":"The FedVision framework is an extensible framework. When the master receives the job submit request, it will use different job loader according to the job_type parameter. This makes it possible to extent FedVision framework with various machine learning frameworks. Currently, only PaddlePaddle supported with extension configuration file: PaddleFL : jobs : - name : paddle_fl schema : ../schema/paddle_fl.json loader : fedvision.paddle_fl.job:PaddleFLJob tasks : - name : fl_trainer loader : fedvision.paddle_fl.tasks.task.trainer:FLTrainer - name : fl_aggregator loader : fedvision.paddle_fl.tasks.task.aggregator:FLAggregator To implement an new extension, one need to add configuration to extensions.yaml implement abstract job class implement several abstract task class used by job.","title":"Job's Life Cycle"},{"location":"framework/paddledetection/","text":"Federated Job using PaddleDetection and PaddleFL are supported out of box. To submit a PaddleDetection jobs, one need a yaml config file to describe what algorithm and parameters to used. This is almost same as these paddle detection configs except that reference other config file are not supported. One can find an example using yolo mobilenet to detect fruit data in examples .","title":"paddledetection"},{"location":"framework/paddlefl/","text":"We provide a mnist demo to describes how to implement a job directly from PaddleFL . There are two py file related: fl_master : define CNN network to learning mnist dataset and compiled with PaddleFL fl_trainer : read dataset and training with compiled program. Just implement your demo by mimic this demo as you wish.","title":"paddlefl"},{"location":"quickstart/quickstart/","text":"This section describes how to quick deploy and run examples in standalone version of FedVision Prerequisites \u00b6 Too run Fedvision, following dependency or tools required: machine to install fedvision-deploy-toolkit: python virtualenv with Python>=3 setup SSH password less login to machine(s) for deploy fedvision framework. machine(s) to deploy fedvision framework: Python>=3.7(with pip) an isolated directory (each directory will be deployed with a copy of code) Deploy \u00b6 // create and activate python virtual environment $ python3 -V Python 3.9.0 $ python3 -m venv venv && source venv/bin/activate // install fedvision_deploy_toolkit $ ( venv ) python -m pip install -U pip && python -m pip install fedvision_deploy_toolkit ---> 100% Successfully installed fedvision_deploy_toolkit // generate deploy template $ ( venv ) fedvision-deploy template standalone // read comments in generated template standalone_template.yaml` and modify as you want. // deploy now $ ( venv ) fedvision-deploy deploy deploy standalone_template.yaml deploying 2 machines: ['machine1'] ---> 100% deploy done Note Deploying Cluster version is almost same except that you should generate template using fedvision-deploy template template and modify generated template file according to comments. Services start \u00b6 Services could be start/stop with scripts in Fedvision/sbin or, use fedvision deploy toolkits: // start services $ fedvision-deploy services all start standalone_template.yaml staring coordinator coordinator1 coordinator service start successfully. pid: 92869 127.0.0.1:22:/data/projects/fedvision started coordinator: port=10000 start coordinator coordinator1 done starting cluster cluster1 clustermanager service start successfully. pid: 92907 127.0.0.1:22:/data/projects/fedvision started cluster manager: port=10001 start cluster cluster1 done, success: True starting cluster workers for cluster cluster1 starting worker worker1 cluster worker service start successfully. pid: 92944 127.0.0.1:22:/data/projects/fedvision started cluster worker: name=worker1 start worker worker1 done, success: True starting master master1 master service start successfully. pid: 92975 127.0.0.1:22:/data/projects/fedvision started master: port=10002 start master master1 done, success: True starting master master2 master service start successfully. pid: 93022 127.0.0.1:22:/data/projects/fedvision started master: port=10003 start master master2 done, success: True starting master master3 master service start successfully. pid: 93067 127.0.0.1:22:/data/projects/fedvision started master: port=10004 start master master3 done, success: True starting master master4 master service start successfully. pid: 93112 127.0.0.1:22:/data/projects/fedvision started master: port=10005 start master master4 done, success: True Run examples \u00b6 Jobs could be submitted at each deployed machine with master service started. $ cd /data/projects/fedvision $ source venv/bin/activate $ export PYTHONPATH = $PYTHONPATH :/data/projects/fedvision/FedVision // submit jobs to master1 $ sh FedVision/examples/paddle_mnist/run.sh 127 .0.0.1:10002 { \"job_id\": \"master1-20201218202835-1\" } Note : find logs in /data/projects/fedvision/FedVision/logs","title":"Quickstart"},{"location":"quickstart/quickstart/#prerequisites","text":"Too run Fedvision, following dependency or tools required: machine to install fedvision-deploy-toolkit: python virtualenv with Python>=3 setup SSH password less login to machine(s) for deploy fedvision framework. machine(s) to deploy fedvision framework: Python>=3.7(with pip) an isolated directory (each directory will be deployed with a copy of code)","title":"Prerequisites"},{"location":"quickstart/quickstart/#deploy","text":"// create and activate python virtual environment $ python3 -V Python 3.9.0 $ python3 -m venv venv && source venv/bin/activate // install fedvision_deploy_toolkit $ ( venv ) python -m pip install -U pip && python -m pip install fedvision_deploy_toolkit ---> 100% Successfully installed fedvision_deploy_toolkit // generate deploy template $ ( venv ) fedvision-deploy template standalone // read comments in generated template standalone_template.yaml` and modify as you want. // deploy now $ ( venv ) fedvision-deploy deploy deploy standalone_template.yaml deploying 2 machines: ['machine1'] ---> 100% deploy done Note Deploying Cluster version is almost same except that you should generate template using fedvision-deploy template template and modify generated template file according to comments.","title":"Deploy"},{"location":"quickstart/quickstart/#services-start","text":"Services could be start/stop with scripts in Fedvision/sbin or, use fedvision deploy toolkits: // start services $ fedvision-deploy services all start standalone_template.yaml staring coordinator coordinator1 coordinator service start successfully. pid: 92869 127.0.0.1:22:/data/projects/fedvision started coordinator: port=10000 start coordinator coordinator1 done starting cluster cluster1 clustermanager service start successfully. pid: 92907 127.0.0.1:22:/data/projects/fedvision started cluster manager: port=10001 start cluster cluster1 done, success: True starting cluster workers for cluster cluster1 starting worker worker1 cluster worker service start successfully. pid: 92944 127.0.0.1:22:/data/projects/fedvision started cluster worker: name=worker1 start worker worker1 done, success: True starting master master1 master service start successfully. pid: 92975 127.0.0.1:22:/data/projects/fedvision started master: port=10002 start master master1 done, success: True starting master master2 master service start successfully. pid: 93022 127.0.0.1:22:/data/projects/fedvision started master: port=10003 start master master2 done, success: True starting master master3 master service start successfully. pid: 93067 127.0.0.1:22:/data/projects/fedvision started master: port=10004 start master master3 done, success: True starting master master4 master service start successfully. pid: 93112 127.0.0.1:22:/data/projects/fedvision started master: port=10005 start master master4 done, success: True","title":"Services start"},{"location":"quickstart/quickstart/#run-examples","text":"Jobs could be submitted at each deployed machine with master service started. $ cd /data/projects/fedvision $ source venv/bin/activate $ export PYTHONPATH = $PYTHONPATH :/data/projects/fedvision/FedVision // submit jobs to master1 $ sh FedVision/examples/paddle_mnist/run.sh 127 .0.0.1:10002 { \"job_id\": \"master1-20201218202835-1\" } Note : find logs in /data/projects/fedvision/FedVision/logs","title":"Run examples"},{"location":"release/change_log/","text":"","title":"Changelog"}]}